#wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/Data/ml-basics/grades.csv

#numpy part 
data = [50,50,47,97,49,3,53,42,26,74,82,62,37,15,70,27,36,35,48,52,63,64]
print(data)


import numpy as np
grades = np.array(data)
print(grades)


print (type(data),'x 2:', data * 2)
print('---')
print (type(grades),'x 2:', grades * 2)
#Note that multiplying a list by 2 creates a new list of twice the length with the original sequence of list elements repeated. Multiplying a NumPy array on the other hand performs an element-wise calculation in which the array behaves like a vector, so we end up with an array of the same size in which each element has been multiplied by 2.
#You might have spotted that the class type for the numpy array above is a numpy.ndarray. The nd indicates that this is a structure that can consists of multiple dimensions (it can have n dimensions). Our specific instance has a single dimension of student grades.
grades.shape

#The shape confirms that this array has only one dimension, which contains 22 elements (there are 22 grades in the original list)


grades.mean()
#average grade

#Define an array of study hours
study_hours = [10.0,11.5,9.0,16.0,9.25,1.0,11.5,9.0,8.5,14.5,15.5,13.75,9.0,8.0,15.5,8.0,9.0,6.0,10.0,12.0,12.5,12.0]

# Create a 2D array (an array of arrays)
student_data = np.array([study_hours, grades])

# display the array
student_data

#While NumPy provides a lot of the functionality you need to work with numbers, and specifically arrays of numeric values; when you start to deal with two-dimensional tables of data, the Pandas package offers a more convenient structure to work with - the DataFrame.
import pandas as pd
df_students = pd.read_csv('grades.csv',delimiter=',',header='infer')
df_students.head()
df_students = pd.DataFrame({'Name': ['Dan', 'Joann', 'Pedro', 'Rosie', 'Ethan', 'Vicky', 'Frederic', 'Jimmie', 
                                     'Rhonda', 'Giovanni', 'Francesca', 'Rajab', 'Naiyana', 'Kian', 'Jenny',
                                     'Jakeem','Helena','Ismat','Anila','Skye','Daniel','Aisha'],
                            'StudyHours':student_data[0],
                            'Grade':student_data[1]})

df_students
#Note that in addition to the columns you specified, the DataFrame includes an index to unique identify each row. We could have specified the index explicitly, and assigned any kind of appropriate value (for example, an email address); but because we didn't specify an index, one has been created with a unique integer value for each row.

# Get the data for index value 5
df_students.loc[5]


# Get the rows with index values from 0 to 5
df_students.loc[0:5]

#In addition to being able to use the loc method to find rows based on the index, you can use the iloc method to find rows based on their ordinal position in the DataFrame (regardless of the index):
# Get data in the first five rows
df_students.iloc[0:5]

#The loc method returned rows with index label in the list of values from 0 to 5 - which includes 0, 1, 2, 3, 4, and 5 (six rows). However, the iloc method returns the rows in the positions included in the range 0 to 5, and since integer ranges don't include the upper-bound value, this includes positions 0, 1, 2, 3, and 4 (five rows).

# find the values for the columns in positions 1 and 2 in row 0, like this:
df_students.iloc[0,[1,2]]


#loc index values
#iloc index position

df_students.loc[0,'Grade'] #return 50

#3 same ways
#filtering with loc 
df_students.loc[df_students['Name']=='Aisha']
#filtering without loc
df_students[df_students['Name']=='Aisha']
#filtering with query
df_students.query('Name=="Aisha"')


#same thing
df_students[df_students.Name == 'Aisha']
df_students[df_students['Name'] == 'Aisha']


#handling missing values
#You can use the isnull method to identify which individual values are null, like this:
df_students.isnull()

#Of course, with a larger DataFrame, it would be inefficient to review all of the rows and columns individually; so we can get the sum of missing values for each column, like this:
df_students.isnull().sum()

#To see them in context, we can filter the dataframe to include only rows where any of the columns (axis 1 of the DataFrame) are null.
df_students[df_students.isnull().any(axis=1)]


#replace the missing value with the mean study hours
df_students.StudyHours = df_students.StudyHours.fillna(df_students.StudyHours.mean())
df_students

#drop rows or columns that contains null values by using the dropna method. In this case, we'll remove rows (axis 0 of the DataFrame) where any of the columns contain null values.

df_students = df_students.dropna(axis=0, how='any')
df_students


####exploring data in the DataFrame


# Get the mean study hours using to column name as an index
mean_study = df_students['StudyHours'].mean()

# Get the mean grade using the column name as a property (just to make the point!)
mean_grade = df_students.Grade.mean()

# Print the mean study hours and mean grade
print('Average weekly study hours: {:.2f}\nAverage grade: {:.2f}'.format(mean_study, mean_grade))

# Get students who studied for the mean or more hours
df_students[df_students.StudyHours > mean_study]

# What was their mean grade?
df_students[df_students.StudyHours > mean_study].Grade.mean()


#Let's assume that the passing grade for the course is 60.
#We can use that information to add a new column to the DataFrame, indicating whether or not each student passed.
#First, we'll create a Pandas Series containing the pass/fail indicator (True or False), and then we'll concatenate that series as a new column (axis 1) in the DataFrame.
passes  = pd.Series(df_students['Grade'] >= 60)
df_students = pd.concat([df_students, passes.rename("Pass")], axis=1)

#For example, you can use the groupby method to group the student data into groups based on the Pass column you added previously, and count the number of names in each group - in other words, you can determine how many students passed and failed
print(df_students.groupby(df_students.Pass).Name.count())

#You can aggregate multiple fields in a group using any available aggregation function. For example, you can find the mean study time and grade for the groups of students who passed and failed the course.
print(df_students.groupby(df_students.Pass)['StudyHours', 'Grade'].mean())

# Create a DataFrame with the data sorted by Grade (descending)
df_students = df_students.sort_values('Grade', ascending=False)

# Show the DataFrame
df_students